{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from athnlp.readers.brown_pos_corpus import BrownPosTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = BrownPosTag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 1, 0, 3, 8, 0, 3, 7, 8, 0, 3, 8, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[795, 612, 356, 17, 1006, 12268, 10, 78, 164, 5029, 78, 1568, 15]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vectors(sentence):\n",
    "    word_vector_size = len(sentence.dictionary.x_dict)\n",
    "    label_vector_size = len(sentence.dictionary.y_dict)\n",
    "    word_vectors = []\n",
    "    label_vectors = []\n",
    "    for label_idx, word_idx in zip(sentence.y, sentence.x):\n",
    "        word_vector = np.zeros(word_vector_size)\n",
    "        word_vector[word_idx] = 1\n",
    "        word_vectors.append(word_vector)\n",
    "        label_vector = np.zeros(label_vector_size)\n",
    "        label_vector[label_idx] = 1\n",
    "        label_vectors.append(label_vector)\n",
    "    return word_vectors, label_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_features(word):\n",
    "    word_features = []\n",
    "    \n",
    "    feature_functions = [\n",
    "       lambda w: w.endswith('ed'),\n",
    "       lambda w: w.isdigit(),\n",
    "       lambda w: w[0].isupper(),\n",
    "       # get creative!\n",
    "    ]\n",
    "    \n",
    "    for feature_fn in feature_functions:\n",
    "        if feature_fn(word):\n",
    "            word_features.append(1)\n",
    "        else:\n",
    "            word_features.append(0)\n",
    "    \n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n",
      "[1, 0, 0]\n",
      "[0, 1, 0]\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for word in ['hello', 'baked', '123', 'Abc']:\n",
    "    print(extract_word_features(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_ouzo_sentence_to_vectors(sentence):\n",
    "    word_vector_size = len(sentence.dictionary.x_dict)\n",
    "    label_vector_size = len(sentence.dictionary.y_dict)\n",
    "    word_vectors = []\n",
    "    label_vectors = []\n",
    "    for i in range(len(sentence.x)):\n",
    "        word_idx = sentence.x[i]\n",
    "        word_vector = np.zeros(word_vector_size)\n",
    "        word_vector[word_idx] = 1\n",
    "        word = sentence.dictionary.x_dict.get_label_name(word_idx)\n",
    "        word_features = extract_word_features(word)\n",
    "        word_vector = np.hstack([word_vector, word_features])\n",
    "        word_vectors.append(word_vector)\n",
    "        \n",
    "        label_vector = np.zeros(label_vector_size)\n",
    "        label_idx = sentence.y[i]\n",
    "        label_vector[label_idx] = 1\n",
    "        label_vectors.append(label_vector)\n",
    "    return word_vectors, label_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vectors_multihot(sentence):\n",
    "    word_vector_size = len(sentence.dictionary.x_dict)\n",
    "    label_vector_size = len(sentence.dictionary.y_dict)\n",
    "    word_vectors = []\n",
    "    label_vectors = []\n",
    "    for i in range(len(sentence.x)):\n",
    "        word_idx = sentence.x[i]\n",
    "        word_vector = np.zeros(word_vector_size)\n",
    "        word_vector[word_idx] = 1\n",
    "        \n",
    "        try:\n",
    "            word_vector[sentence.x[i+1]] = 0.05\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            word_vector[sentence.x[i-1]] = 0.05\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            word_vector[sentence.x[i+2]] = 0.01\n",
    "        except IndexError:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            word_vector[sentence.x[i-2]] = 0.01\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        word_vectors.append(word_vector)\n",
    "        \n",
    "        label_vector = np.zeros(label_vector_size)\n",
    "        label_idx = sentence.y[i]\n",
    "        label_vector[label_idx] = 1\n",
    "        label_vectors.append(label_vector)\n",
    "    return word_vectors, label_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset, sentence_to_vector_mapping_fn=sentence_to_vectors):\n",
    "    word_vectors = []\n",
    "    label_vectors = []\n",
    "    for sentence in dataset:\n",
    "        tmp_word_vectors, tmp_label_vectors = sentence_to_vector_mapping_fn(sentence)\n",
    "        word_vectors.extend(tmp_word_vectors)\n",
    "        label_vectors.extend(tmp_label_vectors)\n",
    "    return word_vectors, label_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word_vector, weight_vectors):\n",
    "    dot_products = calculate_dotproducts(word_vector, weight_vectors)\n",
    "    return np.argmax(dot_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dotproducts(word_vector, weight_vectors):\n",
    "    dot_products = []\n",
    "    for weight_vector in weight_vectors:\n",
    "        result = np.dot(weight_vector, word_vector)\n",
    "        dot_products.append(result)\n",
    "    return dot_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(word_vectors, label_vectors, weight_vectors):\n",
    "    for word_vec, label_vec in zip(word_vectors, label_vectors):\n",
    "\n",
    "        predicted_label_idx = predict(word_vec, weight_vectors)\n",
    "        true_label_idx = np.argmax(label_vec)\n",
    "        \n",
    "        if predicted_label_idx is not true_label_idx:\n",
    "            weight_vectors[predicted_label_idx] -= word_vec\n",
    "            weight_vectors[true_label_idx] += word_vec        \n",
    "        \n",
    "    return weight_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(word_vectors, label_vectors, weight_vectors):\n",
    "    all_predictions = [predict(wv, weight_vectors) for wv in word_vectors]\n",
    "    all_true_labels = [np.argmax(lv) for lv in label_vectors]\n",
    "    return accuracy_score(all_predictions, all_true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(train_word_vecs, train_label_vecs, test_word_vecs, test_label_vecs):\n",
    "    num_features = len(train_word_vecs[0])\n",
    "    num_labels = len(train_label_vecs[0])\n",
    "    \n",
    "    print('train on train, test on test')\n",
    "    train_weights = [np.zeros(num_features) for _ in range(num_labels)]\n",
    "    for epoch in range(4):\n",
    "        train_weights = train(train_word_vecs, train_label_vecs, train_weights)\n",
    "        accuracy = calculate_accuracy(test_word_vecs, test_label_vecs, train_weights)\n",
    "        print(epoch, accuracy)\n",
    "\n",
    "    print('train on train, test on test, randomize order')\n",
    "    train_weights = [np.zeros(num_features) for _ in range(num_labels)]\n",
    "\n",
    "    zipped_samples = zip(train_word_vecs, train_label_vecs)\n",
    "    random_zipped_samples = sorted(zipped_samples, key=lambda k: random.random())\n",
    "    train_word_vecs_random, train_label_vecs_random = zip(*random_zipped_samples)\n",
    "\n",
    "    for epoch in range(4):\n",
    "        train_weights = train(train_word_vecs_random, train_label_vecs_random, train_weights)\n",
    "        accuracy = calculate_accuracy(test_word_vecs, test_label_vecs, train_weights)\n",
    "        print(epoch, accuracy)\n",
    "        \n",
    "    print('train on train, test on test, randomize order, each epoch')\n",
    "    train_weights = [np.zeros(num_features) for _ in range(num_labels)]\n",
    "\n",
    "    for epoch in range(4):\n",
    "\n",
    "        zipped_samples = zip(train_word_vecs, train_label_vecs)\n",
    "        random_zipped_samples = sorted(zipped_samples, key=lambda k: random.random())\n",
    "        train_word_vecs_random, train_label_vecs_random = zip(*random_zipped_samples)\n",
    "\n",
    "        train_weights = train(train_word_vecs_random, train_label_vecs_random, train_weights)\n",
    "        accuracy = calculate_accuracy(test_word_vecs, test_label_vecs, train_weights)\n",
    "        print(epoch, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on train, test on test\n",
      "0 0.8983333333333333\n",
      "1 0.8983333333333333\n",
      "2 0.8983333333333333\n",
      "3 0.8983333333333333\n",
      "train on train, test on test, randomize order\n",
      "0 0.9001960784313725\n",
      "1 0.9001960784313725\n",
      "2 0.9001960784313725\n",
      "3 0.9001960784313725\n",
      "train on train, test on test, randomize order, each epoch\n",
      "0 0.9003921568627451\n",
      "1 0.9016666666666666\n",
      "2 0.9055882352941177\n",
      "3 0.9077450980392157\n"
     ]
    }
   ],
   "source": [
    "test_word_vecs, test_label_vecs = prepare_data(corpus.test)\n",
    "train_word_vecs, train_label_vecs = prepare_data(corpus.train)\n",
    "run_evaluation(train_word_vecs, train_label_vecs, test_word_vecs, test_label_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on train, test on test\n",
      "0 0.8427450980392157\n",
      "1 0.8394117647058823\n",
      "2 0.8420588235294117\n",
      "3 0.8357843137254902\n",
      "train on train, test on test, randomize order\n",
      "0 0.8356862745098039\n",
      "1 0.835\n",
      "2 0.8281372549019608\n",
      "3 0.8308823529411765\n",
      "train on train, test on test, randomize order, each epoch\n",
      "0 0.8291176470588235\n",
      "1 0.8442156862745098\n",
      "2 0.8441176470588235\n",
      "3 0.8420588235294117\n"
     ]
    }
   ],
   "source": [
    "test_word_vecs, test_label_vecs = prepare_data(corpus.test, sentence_to_vector_mapping_fn=sentence_to_vectors_multihot)\n",
    "train_word_vecs, train_label_vecs = prepare_data(corpus.train, sentence_to_vector_mapping_fn=sentence_to_vectors_multihot)\n",
    "run_evaluation(train_word_vecs, train_label_vecs, test_word_vecs, test_label_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on train, test on test\n",
      "0 0.9047058823529411\n",
      "1 0.9047058823529411\n",
      "2 0.9051960784313725\n",
      "3 0.9072549019607843\n",
      "train on train, test on test, randomize order\n",
      "0 0.9060784313725491\n",
      "1 0.9033333333333333\n",
      "2 0.903235294117647\n",
      "3 0.9023529411764706\n",
      "train on train, test on test, randomize order, each epoch\n",
      "0 0.9110784313725491\n",
      "1 0.9104901960784314\n",
      "2 0.8787254901960785\n",
      "3 0.9067647058823529\n"
     ]
    }
   ],
   "source": [
    "test_word_vecs, test_label_vecs = prepare_data(corpus.test, sentence_to_vector_mapping_fn=super_ouzo_sentence_to_vectors)\n",
    "train_word_vecs, train_label_vecs = prepare_data(corpus.train, sentence_to_vector_mapping_fn=super_ouzo_sentence_to_vectors)\n",
    "run_evaluation(train_word_vecs, train_label_vecs, test_word_vecs, test_label_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
